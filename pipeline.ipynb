{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import LeafSLA # custom functions from LeafSLA.py – must be in same directory as this script (pipeline.ipynb), otherwise you have to use os.chdir to help it find the script\n",
    "\n",
    "os.chdir(\"/Users/kericlamb/Documents/Work MacBook/Research/protocols/phenotyping_drought/leaf_lobing/\") # directory with the file ./LeafSLA.py in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable definitions needed for both pipelines \n",
    "img_folder = \"mim_SLA\" # folder name that has images to process (within the current working directory, e.g., /Users/kericlamb/.../leaf_lobing/photos)\n",
    "sq_size = 1 # size of the square (one side) in the image used as the standard -- I've been using SH=1 and LB=2 (?)\n",
    "\n",
    "# for SAM pipeline\n",
    "model_type = \"vit_b\"  # \"vit_h\" (best), \"vit_l\" (medium), \"vit_b\" (smallest)\n",
    "checkpoint = \"~/Documents/SAM/sam_vit_b_01ec64.pth\"\n",
    "bbox_multiplier = 4 # 2 for Lynn; 4 for Stacy... mess around with it. has to do with size of the red square more than anything\n",
    "leaf_position = \"above\" # where leaf is relative to the red square\n",
    "perspective_shift = False # for SH, False is best. for LB 2023 images, True is best\n",
    "\n",
    "# for algorithmic pipeline\n",
    "saturation = 50 # saturation level. 50 for Stacy H., 70 for Lynn B.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate folders for analysis (as needed)\n",
    "folders = [\"{0}/{1}/color_masks\".format(os.getcwd(), img_folder),\n",
    "           \"{0}/{1}/processed\".format(os.getcwd(), img_folder),\n",
    "           \"{0}/{1}/bboxes\".format(os.getcwd(), img_folder),\n",
    "           \"{0}/{1}/perspective\".format(os.getcwd(), img_folder),\n",
    "           \"{0}/{1}/threshold\".format(os.getcwd(), img_folder)]\n",
    "\n",
    "if not os.path.exists(\"{0}/data\".format(os.getcwd())):\n",
    "    os.makedirs(\"{0}/data\".format(os.getcwd())) # checks data folder separately\n",
    "if not os.path.exists(folders[0]): # checks color_masks, processed, shadow, and threshold\n",
    "    # Create all folders\n",
    "    for folder in folders:\n",
    "        os.makedirs(folder)\n",
    "    print(f\"All directories created: {', '.join(folders)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the SAM Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 20240630_205542 - 8 %   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m leaf_array, box_array \u001b[38;5;241m=\u001b[39m LeafSLA\u001b[38;5;241m.\u001b[39msetup_image(img_src\u001b[38;5;241m=\u001b[39mimg_list[i], img_folder\u001b[38;5;241m=\u001b[39mimg_folder, img_name\u001b[38;5;241m=\u001b[39mimg_names[i], color_ranges\u001b[38;5;241m=\u001b[39mLeafSLA\u001b[38;5;241m.\u001b[39mcolor_thresher(), \n\u001b[1;32m     15\u001b[0m                                             sq_size\u001b[38;5;241m=\u001b[39msq_size, leaf_position\u001b[38;5;241m=\u001b[39mleaf_position, bbox_multiplier\u001b[38;5;241m=\u001b[39mbbox_multiplier)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# applies SAM – does not return an object but saves copy to the ./threshold path\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mLeafSLA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAM_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleaf_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./SAM/models/sam_vit_b_01ec64.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# applies minor blurring and edge smoothing (degree of which is controlled by erode and sigma (these are hidden but can be added to the function below))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m binary_image \u001b[38;5;241m=\u001b[39m LeafSLA\u001b[38;5;241m.\u001b[39mimage_repair(img_folder\u001b[38;5;241m=\u001b[39mimg_folder, img_name\u001b[38;5;241m=\u001b[39mimg_names[i])\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/leaf_lobing/code/SLA_pipeline/LeafSLA/LeafSLA.py:189\u001b[0m, in \u001b[0;36mSAM_image\u001b[0;34m(img_src, img_folder, img_name, leaf_array, box_array, model_type, checkpoint, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mSAM_image\u001b[39m(img_src, img_folder, img_name, leaf_array, box_array, model_type, checkpoint, device):\n\u001b[1;32m    188\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(img_src))\n\u001b[0;32m--> 189\u001b[0m     sam \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    190\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m SamPredictor(sam)\n\u001b[1;32m    192\u001b[0m     predictor\u001b[38;5;241m.\u001b[39mset_image(image)\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/micro_phenotyping/grounded_SAM/Grounded-Segment-Anything/segment_anything/segment_anything/build_sam.py:38\u001b[0m, in \u001b[0;36mbuild_sam_vit_b\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_sam_vit_b\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/micro_phenotyping/grounded_SAM/Grounded-Segment-Anything/segment_anything/segment_anything/build_sam.py:67\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m     64\u001b[0m vit_patch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m     65\u001b[0m image_embedding_size \u001b[38;5;241m=\u001b[39m image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m vit_patch_size\n\u001b[1;32m     66\u001b[0m sam \u001b[38;5;241m=\u001b[39m Sam(\n\u001b[0;32m---> 67\u001b[0m     image_encoder\u001b[38;5;241m=\u001b[39m\u001b[43mImageEncoderViT\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_num_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_patch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_rel_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     81\u001b[0m     prompt_encoder\u001b[38;5;241m=\u001b[39mPromptEncoder(\n\u001b[1;32m     82\u001b[0m         embed_dim\u001b[38;5;241m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     83\u001b[0m         image_embedding_size\u001b[38;5;241m=\u001b[39m(image_embedding_size, image_embedding_size),\n\u001b[1;32m     84\u001b[0m         input_image_size\u001b[38;5;241m=\u001b[39m(image_size, image_size),\n\u001b[1;32m     85\u001b[0m         mask_in_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     86\u001b[0m     ),\n\u001b[1;32m     87\u001b[0m     mask_decoder\u001b[38;5;241m=\u001b[39mMaskDecoder(\n\u001b[1;32m     88\u001b[0m         num_multimask_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     89\u001b[0m         transformer\u001b[38;5;241m=\u001b[39mTwoWayTransformer(\n\u001b[1;32m     90\u001b[0m             depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     91\u001b[0m             embedding_dim\u001b[38;5;241m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     92\u001b[0m             mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     93\u001b[0m             num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     94\u001b[0m         ),\n\u001b[1;32m     95\u001b[0m         transformer_dim\u001b[38;5;241m=\u001b[39mprompt_embed_dim,\n\u001b[1;32m     96\u001b[0m         iou_head_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     97\u001b[0m         iou_head_hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     98\u001b[0m     ),\n\u001b[1;32m     99\u001b[0m     pixel_mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m123.675\u001b[39m, \u001b[38;5;241m116.28\u001b[39m, \u001b[38;5;241m103.53\u001b[39m],\n\u001b[1;32m    100\u001b[0m     pixel_std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m58.395\u001b[39m, \u001b[38;5;241m57.12\u001b[39m, \u001b[38;5;241m57.375\u001b[39m],\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m sam\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/micro_phenotyping/grounded_SAM/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:74\u001b[0m, in \u001b[0;36mImageEncoderViT.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, embed_dim, depth, num_heads, mlp_ratio, out_chans, qkv_bias, norm_layer, act_layer, use_abs_pos, use_rel_pos, rel_pos_zero_init, window_size, global_attn_indexes)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth):\n\u001b[0;32m---> 74\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_rel_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_rel_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_pos_zero_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_pos_zero_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mglobal_attn_indexes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mappend(block)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     89\u001b[0m     nn\u001b[38;5;241m.\u001b[39mConv2d(\n\u001b[1;32m     90\u001b[0m         embed_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     LayerNorm2d(out_chans),\n\u001b[1;32m    104\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/micro_phenotyping/grounded_SAM/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:165\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, dim, num_heads, mlp_ratio, qkv_bias, norm_layer, act_layer, use_rel_pos, rel_pos_zero_init, window_size, input_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m Attention(\n\u001b[1;32m    156\u001b[0m     dim,\n\u001b[1;32m    157\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     input_size\u001b[38;5;241m=\u001b[39minput_size \u001b[38;5;28;01mif\u001b[39;00m window_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (window_size, window_size),\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m norm_layer(dim)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLPBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m=\u001b[39m window_size\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/micro_phenotyping/grounded_SAM/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/common.py:22\u001b[0m, in \u001b[0;36mMLPBlock.__init__\u001b[0;34m(self, embedding_dim, mlp_dim, act)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(embedding_dim, mlp_dim)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;241m=\u001b[39m act()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read in images and create lists of the image names sans extension (img_names) and image full paths to the working directory (img_list)\n",
    "img_list, img_names = LeafSLA.img_import(img_folder=img_folder, folders=False)\n",
    "df, area_df = LeafSLA.new_data()\n",
    "\n",
    "# run a for-loop for all images in the specified img_folder\n",
    "for i in range(len(img_names)):\n",
    "    # print only every ten images\n",
    "    if i % 1 == 0: \n",
    "        perc = int((i/len(img_names))*100)\n",
    "        print(f'Processing image: {img_names[i]} - {perc} %   ', end='\\r')\n",
    "    \n",
    "    try: \n",
    "        # reads in image and finds loose bounding boxes for the leaf and red box using color thresholding and saves an image copy to ./bbox path\n",
    "        leaf_array, box_array = LeafSLA.setup_image(img_src=img_list[i], img_folder=img_folder, img_name=img_names[i], color_ranges=LeafSLA.color_thresher(), \n",
    "                                                    sq_size=sq_size, leaf_position=leaf_position, bbox_multiplier=bbox_multiplier)\n",
    "\n",
    "        # applies SAM – does not return an object but saves copy to the ./threshold path\n",
    "        LeafSLA.SAM_image(img_src=img_list[i], img_folder=img_folder, img_name=img_names[i], leaf_array=leaf_array, box_array=box_array, model_type=\"vit_b\", \n",
    "                        checkpoint=\"./SAM/models/sam_vit_b_01ec64.pth\", device=\"mps\")\n",
    "\n",
    "        # applies minor blurring and edge smoothing (degree of which is controlled by erode and sigma (these are hidden but can be added to the function below))\n",
    "        binary_image = LeafSLA.image_repair(img_folder=img_folder, img_name=img_names[i])\n",
    "\n",
    "        # corrects for perspective using the red square and saves a copy to ./perspective path\n",
    "        if perspective_shift == True:\n",
    "            binary_image = LeafSLA.perspective_correction(binary_image, img_folder, img_name=img_names[i])\n",
    "\n",
    "        # measures all relevant phenotypes and saves a copy to ./processed path\n",
    "        df, area_df = LeafSLA.contour_measurement(img_src=img_list[i], img_folder=img_folder, img_name=img_names[i], image=binary_image, sq_size=sq_size,\n",
    "                                                df=df, area_df=area_df, method=\"SAM\")\n",
    "    except Exception as e: \n",
    "        print(f\"{img_names[i]} had the error: {e}\")\n",
    "        print(\"\") # prevents multiple statement prints per line\n",
    "    \n",
    "# save all relevant data for each image AFTER all images have been processed\n",
    "df.to_csv('./data/{0}_metadata.csv'.format(img_folder), index=False)\n",
    "area_df.to_csv('./data/{0}_measurements_sam.csv'.format(img_folder), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Naive Pipeline\n",
    "###### This pipe is more prone to error on difficult images, but works decently on simpler images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240626_140944 had the error: contour_measurement() missing 1 required positional argument: 'method'\n",
      "\n",
      "20240626_142405 had the error: contour_measurement() missing 1 required positional argument: 'method'\n",
      "\n",
      "20240630_205542 had the error: contour_measurement() missing 1 required positional argument: 'method'\n",
      "\n",
      "Processing image: 20240702_190250 - 12 %   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# this pipeline is more prone to error, so nesting in try/except loop that will print error if things go south\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# read in image and apply naïve pipeline\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mLeafSLA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgo_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaturation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaturation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# applies minor blurring and edge smoothing (degree of which is controlled by erode and sigma (these are hidden but can be added to the function below))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     binary_image \u001b[38;5;241m=\u001b[39m LeafSLA\u001b[38;5;241m.\u001b[39mimage_repair(img_folder\u001b[38;5;241m=\u001b[39mimg_folder, img_name\u001b[38;5;241m=\u001b[39mimg_names[i])\n",
      "File \u001b[0;32m~/Documents/Work MacBook/Research/protocols/phenotyping_drought/leaf_lobing/code/SLA_pipeline/LeafSLA/LeafSLA.py:439\u001b[0m, in \u001b[0;36malgo_pipeline\u001b[0;34m(img_src, img_folder, img_name, saturation, crop_tuple, black_white_tuple, small_hole_size)\u001b[0m\n\u001b[1;32m    436\u001b[0m mask \u001b[38;5;241m=\u001b[39m glob_mean\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# Perform morphological reconstruction\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m rec \u001b[38;5;241m=\u001b[39m \u001b[43mskimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphology\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdilation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m binary_objects \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    441\u001b[0m binary_filled \u001b[38;5;241m=\u001b[39m skimage\u001b[38;5;241m.\u001b[39mmorphology\u001b[38;5;241m.\u001b[39mremove_small_holes(binary_objects, small_hole_size)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/skimage/morphology/grayreconstruct.py:212\u001b[0m, in \u001b[0;36mreconstruction\u001b[0;34m(seed, mask, method, footprint, offset)\u001b[0m\n\u001b[1;32m    210\u001b[0m start \u001b[38;5;241m=\u001b[39m index_sorted[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    211\u001b[0m value_rank \u001b[38;5;241m=\u001b[39m value_rank\u001b[38;5;241m.\u001b[39mastype(unsigned_int_dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m \u001b[43mreconstruction_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_strides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_stride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Reshape reconstructed image to original image shape and remove padding.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m rec_img \u001b[38;5;241m=\u001b[39m value_map[value_rank[:image_stride]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read in images and create lists of the image names sans extension (img_names) and image full paths to the working directory (img_list)\n",
    "img_list, img_names = LeafSLA.img_import(img_folder=img_folder, folders=False)\n",
    "df, area_df = LeafSLA.new_data()\n",
    "\n",
    "# run a for-loop for all images in the specified img_folder\n",
    "for i in range(len(img_names)):\n",
    "    # print only every ten images\n",
    "    if i % 1 == 0: \n",
    "        perc = int((i/len(img_names))*100)\n",
    "        print(f'Processing image: {img_names[i]} - {perc} %   ', end='\\r')\n",
    "\n",
    "    # this pipeline is more prone to error, so nesting in try/except loop that will print error if things go south\n",
    "    try: \n",
    "        # read in image and apply naïve pipeline\n",
    "        LeafSLA.algo_pipeline(img_src=img_list[i], img_folder=img_folder, img_name=img_names[i], saturation=saturation)\n",
    "\n",
    "        # applies minor blurring and edge smoothing (degree of which is controlled by erode and sigma (these are hidden but can be added to the function below))\n",
    "        binary_image = LeafSLA.image_repair(img_folder=img_folder, img_name=img_names[i])\n",
    "\n",
    "        # corrects for perspective using the red square and saves a copy to ./perspective path\n",
    "        corrected_image = LeafSLA.perspective_correction(binary_image, img_folder, img_name=img_names[i])\n",
    "\n",
    "        # measures all relevant phenotypes and saves a copy to ./processed path\n",
    "        df, area_df = LeafSLA.contour_measurement(img_src=img_list[i], img_folder=img_folder, img_name=img_names[1], image=corrected_image, sq_size=sq_size,\n",
    "                                                  df=df, area_df=area_df)\n",
    "    except Exception as e: \n",
    "        print(f\"{img_names[i]} had the error: {e}\")\n",
    "        print(\"\") # prevents multiple statement prints per line\n",
    "\n",
    "# save all relevant data for each image AFTER all images have been processed\n",
    "df.to_csv('./data/{0}_metadata.csv'.format(img_folder), index=False)\n",
    "area_df.to_csv('./data/{0}_measurements_sam.csv'.format(img_folder), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
